{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">TensorFlow Neural Network Lab</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/notmnist.png\">\n",
    "In this lab, you'll use all the tools you learned from *Introduction to TensorFlow* to label images of English letters! The data you are using, <a href=\"http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html\">notMNIST</a>, consists of images of a letter from A to J in different fonts.\n",
    "\n",
    "The above images are a few examples of the data you'll be training on. After training the network, you will compare your prediction model against test data. Your goal, by the end of this lab, is to make predictions against that test set with at least an 80% accuracy. Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start this lab, you first need to import all the necessary modules. Run the code below. If it runs successfully, it will print \"`All modules imported`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notMNIST dataset is too large for many computers to handle.  It contains 500,000 images for just training.  You'll be using a subset of this data, 15,000 images for each label (A-J)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files downloaded.\n"
     ]
    }
   ],
   "source": [
    "def download(url, file):\n",
    "    \"\"\"\n",
    "    Download file from <url>\n",
    "    :param url: URL to file\n",
    "    :param file: Local file path\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file):\n",
    "        print('Downloading ' + file + '...')\n",
    "        urlretrieve(url, file)\n",
    "        print('Download Finished')\n",
    "\n",
    "# Download the training and test dataset.\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_train.zip', 'notMNIST_train.zip')\n",
    "download('https://s3.amazonaws.com/udacity-sdc/notMNIST_test.zip', 'notMNIST_test.zip')\n",
    "\n",
    "# Make sure the files aren't corrupted\n",
    "assert hashlib.md5(open('notMNIST_train.zip', 'rb').read()).hexdigest() == 'c8673b3f28f489e9cdf3a3d74e2ac8fa',\\\n",
    "        'notMNIST_train.zip file is corrupted.  Remove the file and try again.'\n",
    "assert hashlib.md5(open('notMNIST_test.zip', 'rb').read()).hexdigest() == '5d3c7e653e63471c88df796156a9dfa9',\\\n",
    "        'notMNIST_test.zip file is corrupted.  Remove the file and try again.'\n",
    "\n",
    "# Wait until you see that all files have been downloaded.\n",
    "print('All files downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/210001 [00:00<?, ?files/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 720/210001 [00:00<00:29, 7138.36files/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 1317/210001 [00:00<00:33, 6275.60files/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 1846/210001 [00:00<00:34, 5955.89files/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 2400/210001 [00:00<00:35, 5853.70files/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 2832/210001 [00:00<00:37, 5515.88files/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 3410/210001 [00:00<00:37, 5564.62files/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 4031/210001 [00:00<00:36, 5666.11files/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 4550/210001 [00:00<00:37, 5482.01files/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 5045/210001 [00:00<00:38, 5377.91files/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 5622/210001 [00:01<00:37, 5415.34files/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 6206/210001 [00:01<00:37, 5451.49files/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 6788/210001 [00:01<00:37, 5480.52files/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 7371/210001 [00:01<00:36, 5506.95files/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 8003/210001 [00:01<00:36, 5563.06files/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 8661/210001 [00:01<00:35, 5628.38files/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 9265/210001 [00:01<00:35, 5639.17files/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 9920/210001 [00:01<00:35, 5690.49files/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 10534/210001 [00:01<00:35, 5670.71files/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 11159/210001 [00:01<00:34, 5705.54files/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 11762/210001 [00:02<00:34, 5715.55files/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 12393/210001 [00:02<00:34, 5742.77files/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 13059/210001 [00:02<00:34, 5782.88files/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 13685/210001 [00:02<00:33, 5786.70files/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 14328/210001 [00:02<00:33, 5812.81files/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 14952/210001 [00:02<00:33, 5829.48files/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 15584/210001 [00:02<00:33, 5846.97files/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 16242/210001 [00:02<00:32, 5874.10files/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 16899/210001 [00:02<00:32, 5897.64files/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 17541/210001 [00:02<00:32, 5914.65files/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 18182/210001 [00:03<00:32, 5924.95files/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 18821/210001 [00:03<00:32, 5938.78files/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 19457/210001 [00:03<00:32, 5934.43files/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 20077/210001 [00:03<00:32, 5932.45files/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 20716/210001 [00:03<00:31, 5942.98files/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 21334/210001 [00:03<00:31, 5949.95files/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 21950/210001 [00:03<00:31, 5912.15files/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 22527/210001 [00:03<00:31, 5885.43files/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 23103/210001 [00:03<00:31, 5882.45files/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 23687/210001 [00:04<00:31, 5881.07files/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 24255/210001 [00:04<00:31, 5868.35files/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 24817/210001 [00:04<00:31, 5856.71files/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 25415/210001 [00:04<00:31, 5863.49files/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 26016/210001 [00:04<00:31, 5866.47files/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 26600/210001 [00:04<00:31, 5863.01files/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 27190/210001 [00:04<00:31, 5868.28files/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 27773/210001 [00:04<00:31, 5846.16files/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 28334/210001 [00:04<00:31, 5810.88files/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 28858/210001 [00:04<00:31, 5795.84files/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 29490/210001 [00:05<00:31, 5806.21files/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 30066/210001 [00:05<00:30, 5805.21files/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 30692/210001 [00:05<00:30, 5813.76files/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 31272/210001 [00:05<00:30, 5812.69files/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 31896/210001 [00:05<00:30, 5820.22files/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 32488/210001 [00:05<00:30, 5801.17files/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 33140/210001 [00:05<00:30, 5812.21files/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 33729/210001 [00:05<00:30, 5812.66files/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 34317/210001 [00:05<00:30, 5810.17files/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 34899/210001 [00:06<00:30, 5809.84files/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 35533/210001 [00:06<00:29, 5818.40files/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 36151/210001 [00:06<00:29, 5823.81files/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 36754/210001 [00:06<00:29, 5823.13files/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 37349/210001 [00:06<00:29, 5821.80files/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 37941/210001 [00:06<00:29, 5823.62files/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 38549/210001 [00:06<00:29, 5827.55files/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 39211/210001 [00:06<00:29, 5839.16files/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 39826/210001 [00:06<00:29, 5839.59files/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 40433/210001 [00:06<00:29, 5839.79files/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 41033/210001 [00:07<00:28, 5832.85files/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 41616/210001 [00:07<00:28, 5828.70files/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 42191/210001 [00:07<00:28, 5823.97files/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 42758/210001 [00:07<00:28, 5813.43files/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 43361/210001 [00:07<00:28, 5816.17files/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 44012/210001 [00:07<00:28, 5825.45files/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██▏       | 44635/210001 [00:07<00:28, 5830.57files/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 45235/210001 [00:07<00:28, 5830.69files/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 45830/210001 [00:07<00:28, 5829.16files/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 46420/210001 [00:07<00:28, 5827.17files/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 47016/210001 [00:08<00:27, 5828.57files/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 47644/210001 [00:08<00:27, 5834.11files/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 48277/210001 [00:08<00:27, 5839.97files/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 48891/210001 [00:08<00:27, 5843.57files/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 49502/210001 [00:08<00:27, 5844.72files/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 50144/210001 [00:08<00:27, 5851.38files/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 50760/210001 [00:08<00:27, 5854.84files/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 51380/210001 [00:08<00:27, 5858.64files/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 51997/210001 [00:08<00:27, 5840.41files/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 52616/210001 [00:09<00:26, 5844.16files/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 53202/210001 [00:09<00:26, 5839.38files/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 53775/210001 [00:09<00:26, 5819.30files/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 54309/210001 [00:09<00:26, 5807.09files/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 54826/210001 [00:09<00:26, 5788.15files/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 55318/210001 [00:09<00:26, 5773.27files/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 55797/210001 [00:09<00:26, 5755.38files/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 56330/210001 [00:09<00:26, 5751.54files/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 56821/210001 [00:09<00:26, 5742.95files/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 57331/210001 [00:09<00:26, 5736.48files/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 57828/210001 [00:10<00:26, 5727.99files/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 58325/210001 [00:10<00:26, 5720.01files/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 58819/210001 [00:10<00:26, 5712.90files/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 59313/210001 [00:10<00:26, 5705.31files/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 59806/210001 [00:10<00:26, 5696.23files/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 60303/210001 [00:10<00:26, 5689.22files/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 60794/210001 [00:10<00:26, 5675.31files/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 61269/210001 [00:10<00:26, 5666.03files/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 61773/210001 [00:10<00:26, 5660.22files/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 62283/210001 [00:11<00:26, 5654.88files/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 62807/210001 [00:11<00:26, 5651.12files/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 63307/210001 [00:11<00:26, 5639.65files/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 63792/210001 [00:11<00:25, 5630.68files/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 64292/210001 [00:11<00:25, 5625.06files/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 64787/210001 [00:11<00:25, 5618.77files/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 65275/210001 [00:11<00:25, 5608.69files/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 65769/210001 [00:11<00:25, 5602.98files/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 66257/210001 [00:11<00:25, 5596.83files/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 66751/210001 [00:11<00:25, 5591.35files/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 67238/210001 [00:12<00:25, 5582.55files/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 67718/210001 [00:12<00:25, 5575.49files/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 68195/210001 [00:12<00:25, 5568.89files/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 68691/210001 [00:12<00:25, 5563.76files/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 69174/210001 [00:12<00:25, 5557.85files/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 69690/210001 [00:12<00:25, 5554.62files/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 70207/210001 [00:12<00:25, 5551.54files/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▎      | 70722/210001 [00:12<00:25, 5548.36files/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 71234/210001 [00:12<00:25, 5545.02files/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 71787/210001 [00:12<00:24, 5544.86files/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 72339/210001 [00:13<00:24, 5544.61files/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 72869/210001 [00:13<00:24, 5539.12files/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 73387/210001 [00:13<00:24, 5533.83files/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 73896/210001 [00:13<00:24, 5526.32files/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 74391/210001 [00:13<00:24, 5511.25files/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 74858/210001 [00:13<00:24, 5501.20files/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 75347/210001 [00:13<00:24, 5496.70files/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 75811/210001 [00:13<00:24, 5487.27files/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▋      | 76314/210001 [00:13<00:24, 5484.73files/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 76783/210001 [00:14<00:24, 5478.40files/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 77308/210001 [00:14<00:24, 5476.15files/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 77792/210001 [00:14<00:24, 5470.34files/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 78272/210001 [00:14<00:24, 5465.10files/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 78750/210001 [00:14<00:24, 5459.62files/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 79247/210001 [00:14<00:23, 5456.31files/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 79729/210001 [00:14<00:23, 5448.47files/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 80199/210001 [00:14<00:23, 5443.11files/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 80668/210001 [00:14<00:23, 5438.48files/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▊      | 81137/210001 [00:14<00:23, 5432.35files/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 81636/210001 [00:15<00:23, 5429.36files/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 82111/210001 [00:15<00:23, 5423.74files/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 82611/210001 [00:15<00:23, 5420.96files/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 83091/210001 [00:15<00:23, 5410.67files/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 83568/210001 [00:15<00:23, 5406.57files/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 84066/210001 [00:15<00:23, 5403.46files/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 84540/210001 [00:15<00:23, 5398.55files/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 85036/210001 [00:15<00:23, 5395.70files/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 85541/210001 [00:15<00:23, 5393.54files/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 86027/210001 [00:15<00:23, 5389.66files/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 86551/210001 [00:16<00:22, 5388.71files/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████▏     | 87047/210001 [00:16<00:22, 5385.62files/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 87541/210001 [00:16<00:22, 5381.69files/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 88030/210001 [00:16<00:22, 5378.62files/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 88519/210001 [00:16<00:22, 5372.26files/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 89040/210001 [00:16<00:22, 5371.03files/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 89529/210001 [00:16<00:22, 5366.73files/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 90039/210001 [00:16<00:22, 5364.88files/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 90530/210001 [00:16<00:22, 5361.67files/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 91046/210001 [00:16<00:22, 5359.99files/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▎     | 91542/210001 [00:17<00:22, 5356.56files/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 92036/210001 [00:17<00:22, 5354.14files/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 92538/210001 [00:17<00:21, 5349.41files/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 93032/210001 [00:17<00:21, 5344.60files/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 93522/210001 [00:17<00:21, 5342.05files/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 94023/210001 [00:17<00:21, 5340.08files/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 94540/210001 [00:17<00:21, 5339.17files/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 95035/210001 [00:17<00:21, 5336.77files/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 95543/210001 [00:17<00:21, 5334.34files/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 96041/210001 [00:18<00:21, 5331.04files/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 96531/210001 [00:18<00:21, 5326.56files/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 97022/210001 [00:18<00:21, 5323.70files/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▋     | 97506/210001 [00:18<00:21, 5318.84files/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 97977/210001 [00:18<00:21, 5310.98files/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 98443/210001 [00:18<00:21, 5307.36files/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 98899/210001 [00:18<00:20, 5303.39files/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 99365/210001 [00:18<00:20, 5299.74files/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 99841/210001 [00:18<00:20, 5296.65files/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 100305/210001 [00:18<00:20, 5291.68files/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 100761/210001 [00:19<00:20, 5285.49files/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 101249/210001 [00:19<00:20, 5283.44files/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 101713/210001 [00:19<00:20, 5279.60files/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▊     | 102206/210001 [00:19<00:20, 5278.06files/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 102675/210001 [00:19<00:20, 5274.70files/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 103165/210001 [00:19<00:20, 5272.64files/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 103640/210001 [00:19<00:20, 5270.32files/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 104113/210001 [00:19<00:20, 5265.34files/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 104575/210001 [00:19<00:20, 5259.26files/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 105044/210001 [00:19<00:19, 5256.30files/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 105561/210001 [00:20<00:19, 5255.95files/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 106035/210001 [00:20<00:19, 5251.46files/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 106499/210001 [00:20<00:19, 5247.77files/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 107012/210001 [00:20<00:19, 5247.18files/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 107543/210001 [00:20<00:19, 5247.44files/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████▏    | 108036/210001 [00:20<00:19, 5245.15files/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 108554/210001 [00:20<00:19, 5244.80files/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 109052/210001 [00:20<00:19, 5241.40files/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 109541/210001 [00:20<00:19, 5239.70files/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 110029/210001 [00:21<00:19, 5236.59files/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 110510/210001 [00:21<00:19, 5233.59files/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 111008/210001 [00:21<00:18, 5232.37files/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 111490/210001 [00:21<00:18, 5230.46files/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 111973/210001 [00:21<00:18, 5228.38files/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▎    | 112455/210001 [00:21<00:18, 5222.96files/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 112974/210001 [00:21<00:18, 5222.77files/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 113453/210001 [00:21<00:18, 5218.99files/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 113982/210001 [00:21<00:18, 5219.27files/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 114493/210001 [00:21<00:18, 5218.72files/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 114988/210001 [00:22<00:18, 5217.49files/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 115482/210001 [00:22<00:18, 5215.94files/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 115988/210001 [00:22<00:18, 5215.20files/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 116485/210001 [00:22<00:17, 5208.77files/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 116959/210001 [00:22<00:17, 5201.76files/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 117417/210001 [00:22<00:17, 5199.95files/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 117872/210001 [00:22<00:17, 5194.39files/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▋    | 118307/210001 [00:22<00:17, 5188.03files/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 118729/210001 [00:22<00:17, 5180.77files/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 119157/210001 [00:23<00:17, 5178.36files/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 119651/210001 [00:23<00:17, 5176.57files/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 120108/210001 [00:23<00:17, 5174.20files/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 120608/210001 [00:23<00:17, 5173.01files/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 121070/210001 [00:23<00:17, 5170.51files/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 121529/210001 [00:23<00:17, 5166.59files/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 121982/210001 [00:23<00:17, 5164.18files/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 122487/210001 [00:23<00:16, 5163.34files/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▊    | 122955/210001 [00:23<00:16, 5161.09files/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 123440/210001 [00:23<00:16, 5159.39files/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 123913/210001 [00:24<00:16, 5157.94files/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 124384/210001 [00:24<00:16, 5154.85files/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 124885/210001 [00:24<00:16, 5154.22files/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 125363/210001 [00:24<00:16, 5152.69files/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 125881/210001 [00:24<00:16, 5153.13files/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 126373/210001 [00:24<00:16, 5151.63files/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 126863/210001 [00:24<00:16, 5149.66files/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 127345/210001 [00:24<00:16, 5147.20files/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 127820/210001 [00:24<00:15, 5144.86files/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 128311/210001 [00:24<00:15, 5143.90files/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████▏   | 128787/210001 [00:25<00:15, 5141.95files/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 129261/210001 [00:25<00:15, 5139.24files/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 129728/210001 [00:25<00:15, 5137.46files/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 130229/210001 [00:25<00:15, 5136.87files/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 130705/210001 [00:25<00:15, 5135.43files/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 131228/210001 [00:25<00:15, 5135.76files/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 131719/210001 [00:25<00:15, 5134.85files/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 132209/210001 [00:25<00:15, 5133.64files/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 132708/210001 [00:25<00:15, 5133.09files/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 133199/210001 [00:25<00:14, 5126.84files/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▎   | 133666/210001 [00:26<00:14, 5124.40files/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 134133/210001 [00:26<00:14, 5122.69files/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 134592/210001 [00:26<00:14, 5117.84files/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 135033/210001 [00:26<00:14, 5113.66files/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 135465/210001 [00:26<00:14, 5110.75files/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 135895/210001 [00:26<00:14, 5106.46files/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 136327/210001 [00:26<00:14, 5103.42files/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 136838/210001 [00:26<00:14, 5103.47files/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 137295/210001 [00:26<00:14, 5100.92files/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 137777/210001 [00:27<00:14, 5100.06files/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 138258/210001 [00:27<00:14, 5099.03files/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 138725/210001 [00:27<00:13, 5097.04files/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▋   | 139190/210001 [00:27<00:13, 5093.90files/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▋   | 139645/210001 [00:27<00:13, 5086.92files/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 140070/210001 [00:27<00:13, 5083.27files/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 140490/210001 [00:27<00:13, 5078.11files/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 140923/210001 [00:27<00:13, 5075.00files/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 141338/210001 [00:27<00:13, 5069.09files/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 141736/210001 [00:27<00:13, 5062.89files/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 142168/210001 [00:28<00:13, 5060.26files/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 142597/210001 [00:28<00:13, 5057.46files/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 143067/210001 [00:28<00:13, 5055.81files/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 143494/210001 [00:28<00:13, 5053.72files/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▊   | 143926/210001 [00:28<00:13, 5051.03files/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▊   | 144354/210001 [00:28<00:13, 5048.30files/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 144826/210001 [00:28<00:12, 5046.95files/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 145310/210001 [00:28<00:12, 5046.37files/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 145771/210001 [00:28<00:12, 5044.88files/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 146264/210001 [00:28<00:12, 5044.34files/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 146731/210001 [00:29<00:12, 5041.36files/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 147186/210001 [00:29<00:12, 5036.73files/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 147623/210001 [00:29<00:12, 5033.91files/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 148056/210001 [00:29<00:12, 5029.94files/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 148501/210001 [00:29<00:12, 5027.92files/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 148930/210001 [00:29<00:12, 5024.38files/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 149377/210001 [00:29<00:12, 5022.41files/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████▏  | 149806/210001 [00:29<00:11, 5019.30files/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 150241/210001 [00:29<00:11, 5016.56files/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 150706/210001 [00:30<00:11, 5015.77files/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 151144/210001 [00:30<00:11, 5012.26files/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 151577/210001 [00:30<00:11, 5010.08files/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 152007/210001 [00:30<00:11, 5007.39files/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 152434/210001 [00:30<00:11, 5004.67files/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 152881/210001 [00:30<00:11, 5002.96files/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 153319/210001 [00:30<00:11, 5000.88files/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 153753/210001 [00:30<00:11, 4998.36files/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 154186/210001 [00:30<00:11, 4995.25files/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▎  | 154662/210001 [00:30<00:11, 4994.47files/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 155109/210001 [00:31<00:10, 4992.83files/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 155560/210001 [00:31<00:10, 4991.10files/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 156004/210001 [00:31<00:10, 4989.37files/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 156447/210001 [00:31<00:10, 4986.49files/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 156898/210001 [00:31<00:10, 4984.94files/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 157345/210001 [00:31<00:10, 4983.30files/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 157787/210001 [00:31<00:10, 4979.54files/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 158214/210001 [00:31<00:10, 4976.78files/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 158673/210001 [00:31<00:10, 4975.52files/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 159174/210001 [00:31<00:10, 4975.62files/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 159628/210001 [00:32<00:10, 4972.76files/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 160072/210001 [00:32<00:10, 4971.09files/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 160531/210001 [00:32<00:09, 4969.71files/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 160997/210001 [00:32<00:09, 4968.94files/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 161450/210001 [00:32<00:09, 4966.73files/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 161904/210001 [00:32<00:09, 4965.33files/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 162362/210001 [00:32<00:09, 4964.18files/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 162828/210001 [00:32<00:09, 4963.21files/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 163313/210001 [00:32<00:09, 4962.89files/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 163817/210001 [00:33<00:09, 4962.91files/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 164293/210001 [00:33<00:09, 4962.10files/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 164791/210001 [00:33<00:09, 4961.92files/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▊  | 165304/210001 [00:33<00:09, 4962.51files/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 165793/210001 [00:33<00:08, 4962.26files/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 166307/210001 [00:33<00:08, 4962.78files/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 166804/210001 [00:33<00:08, 4962.64files/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 167356/210001 [00:33<00:08, 4964.29files/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 167868/210001 [00:33<00:08, 4963.63files/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 168408/210001 [00:33<00:08, 4964.93files/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 168921/210001 [00:34<00:08, 4965.16files/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 169456/210001 [00:34<00:08, 4966.28files/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 169974/210001 [00:34<00:08, 4962.50files/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 170459/210001 [00:34<00:07, 4960.62files/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████▏ | 170953/210001 [00:34<00:07, 4960.53files/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 171462/210001 [00:34<00:07, 4961.21files/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 171951/210001 [00:34<00:07, 4960.96files/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 172473/210001 [00:34<00:07, 4961.68files/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 172985/210001 [00:34<00:07, 4962.14files/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 173541/210001 [00:34<00:07, 4963.80files/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 174059/210001 [00:35<00:07, 4962.03files/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 174555/210001 [00:35<00:07, 4960.96files/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 175042/210001 [00:35<00:07, 4960.71files/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▎ | 175528/210001 [00:35<00:06, 4959.34files/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 176011/210001 [00:35<00:06, 4959.02files/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 176489/210001 [00:35<00:06, 4958.47files/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 176970/210001 [00:35<00:06, 4958.00files/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 177459/210001 [00:35<00:06, 4957.84files/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 177941/210001 [00:35<00:06, 4957.10files/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 178430/210001 [00:35<00:06, 4956.89files/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 178911/210001 [00:36<00:06, 4955.12files/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 179387/210001 [00:36<00:06, 4954.52files/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 179866/210001 [00:36<00:06, 4954.08files/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 180360/210001 [00:36<00:05, 4954.01files/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 180854/210001 [00:36<00:05, 4954.00files/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▋ | 181338/210001 [00:36<00:05, 4952.90files/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 181818/210001 [00:36<00:05, 4952.45files/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 182295/210001 [00:36<00:05, 4951.05files/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 182791/210001 [00:36<00:05, 4951.04files/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 183267/210001 [00:37<00:05, 4949.82files/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 183812/210001 [00:37<00:05, 4951.16files/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 184304/210001 [00:37<00:05, 4948.64files/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 184781/210001 [00:37<00:05, 4948.14files/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 185253/210001 [00:37<00:05, 4947.20files/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 185745/210001 [00:37<00:04, 4947.10files/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▊ | 186221/210001 [00:37<00:04, 4946.17files/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 186724/210001 [00:37<00:04, 4946.36files/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 187204/210001 [00:37<00:04, 4945.12files/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 187735/210001 [00:37<00:04, 4946.09files/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 188261/210001 [00:38<00:04, 4946.90files/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 188802/210001 [00:38<00:04, 4948.10files/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 189315/210001 [00:38<00:04, 4947.35files/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 189846/210001 [00:38<00:04, 4948.27files/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 190355/210001 [00:38<00:03, 4948.61files/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 190871/210001 [00:38<00:03, 4948.71files/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 191382/210001 [00:38<00:03, 4948.96files/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████▏| 191888/210001 [00:38<00:03, 4947.92files/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 192380/210001 [00:38<00:03, 4947.76files/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 192872/210001 [00:38<00:03, 4946.85files/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 193354/210001 [00:39<00:03, 4946.53files/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 193891/210001 [00:39<00:03, 4947.81files/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 194403/210001 [00:39<00:03, 4948.14files/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 194923/210001 [00:39<00:03, 4948.82files/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 195431/210001 [00:39<00:02, 4948.41files/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 195962/210001 [00:39<00:02, 4949.32files/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▎| 196471/210001 [00:39<00:02, 4949.57files/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 197021/210001 [00:39<00:02, 4950.96files/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 197542/210001 [00:39<00:02, 4951.06files/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 198076/210001 [00:39<00:02, 4952.04files/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▍| 198596/210001 [00:40<00:02, 4951.94files/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▍| 199126/210001 [00:40<00:02, 4952.79files/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 199644/210001 [00:40<00:02, 4951.81files/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 200202/210001 [00:40<00:01, 4953.35files/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 200720/210001 [00:40<00:01, 4953.58files/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 201252/210001 [00:40<00:01, 4954.49files/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 201771/210001 [00:40<00:01, 4954.08files/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▋| 202302/210001 [00:40<00:01, 4954.95files/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 202817/210001 [00:40<00:01, 4955.11files/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 203336/210001 [00:41<00:01, 4955.88files/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 203851/210001 [00:41<00:01, 4955.65files/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 204365/210001 [00:41<00:01, 4956.10files/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 204919/210001 [00:41<00:01, 4957.50files/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 205442/210001 [00:41<00:00, 4956.87files/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 205967/210001 [00:41<00:00, 4957.56files/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 206501/210001 [00:41<00:00, 4958.48files/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▊| 207061/210001 [00:41<00:00, 4960.00files/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 207592/210001 [00:41<00:00, 4960.16files/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 208115/210001 [00:41<00:00, 4960.59files/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 208640/210001 [00:42<00:00, 4961.27files/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████▉| 209162/210001 [00:42<00:00, 4960.81files/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████▉| 209695/210001 [00:42<00:00, 4961.70files/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 210001/210001 [00:42<00:00, 4961.88files/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/10001 [00:00<?, ?files/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 526/10001 [00:00<00:01, 5208.89files/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 879/10001 [00:00<00:02, 4372.02files/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 1289/10001 [00:00<00:02, 4278.25files/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 1744/10001 [00:00<00:01, 4296.67files/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 2073/10001 [00:00<00:02, 3883.08files/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 2382/10001 [00:00<00:02, 3585.51files/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 2669/10001 [00:00<00:02, 3469.20files/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 3072/10001 [00:00<00:01, 3533.02files/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 3533/10001 [00:00<00:01, 3639.95files/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 3892/10001 [00:01<00:01, 3451.67files/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 4250/10001 [00:01<00:01, 3462.92files/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 4581/10001 [00:01<00:01, 3427.29files/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 4942/10001 [00:01<00:01, 3440.55files/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 5424/10001 [00:01<00:01, 3531.33files/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 5915/10001 [00:01<00:01, 3615.57files/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 6328/10001 [00:01<00:01, 3640.90files/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 6848/10001 [00:01<00:00, 3725.54files/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▎  | 7361/10001 [00:01<00:00, 3797.60files/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 7828/10001 [00:02<00:00, 3839.04files/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 8293/10001 [00:02<00:00, 3874.70files/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 8840/10001 [00:02<00:00, 3947.54files/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 9329/10001 [00:02<00:00, 3983.89files/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 9815/10001 [00:02<00:00, 4006.15files/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10001/10001 [00:02<00:00, 4016.64files/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features and labels uncompressed.\n"
     ]
    }
   ],
   "source": [
    "def uncompress_features_labels(file):\n",
    "    \"\"\"\n",
    "    Uncompress features and labels from a zip file\n",
    "    :param file: The zip file to extract the data from\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with ZipFile(file) as zipf:\n",
    "        # Progress Bar\n",
    "        filenames_pbar = tqdm(zipf.namelist(), unit='files')\n",
    "        \n",
    "        # Get features and labels from all files\n",
    "        for filename in filenames_pbar:\n",
    "            #print(filename)\n",
    "            # Check if the file is a directory\n",
    "            if not filename.endswith('/'):\n",
    "                with zipf.open(filename) as image_file:\n",
    "                    image = Image.open(image_file)\n",
    "                    image.load()\n",
    "                    # Load image data as 1 dimensional array\n",
    "                    # We're using float32 to save on memory space\n",
    "                    feature = np.array(image, dtype=np.float32).flatten()\n",
    "\n",
    "                # Get the the letter from the filename.  This is the letter of the image. (first letter)\n",
    "                label = os.path.split(filename)[1][0]\n",
    "                #print(os.path.split(filename)[1]) A0.png   filename e.g. notMNIST_train/A0.png, \n",
    "\n",
    "                features.append(feature)\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Get the features and labels from the zip files\n",
    "train_features, train_labels = uncompress_features_labels('notMNIST_train.zip')\n",
    "test_features, test_labels = uncompress_features_labels('notMNIST_test.zip')\n",
    "\n",
    "# Limit the amount of data to work with a docker container\n",
    "docker_size_limit = 15000\n",
    "train_features, train_labels = resample(train_features, train_labels, n_samples=docker_size_limit)\n",
    "\n",
    "# Set flags for feature engineering.  This will prevent you from skipping an important step.\n",
    "is_features_normal = False\n",
    "is_labels_encod = False\n",
    "\n",
    "# Wait until you see that all features and labels have been uncompressed.\n",
    "print('All features and labels uncompressed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features) #15000 *784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3917647 , 0.64588237, 0.60823536, 0.6145098 , 0.6145098 ,\n",
       "       0.6145098 , 0.6145098 , 0.6145098 , 0.6145098 , 0.6113726 ,\n",
       "       0.6333334 , 0.54235303, 0.1       , 0.10313725, 0.1       ,\n",
       "       0.14705883, 0.57058823, 0.62392163, 0.6113726 , 0.6145098 ,\n",
       "       0.6113726 , 0.60823536, 0.60509807, 0.60509807, 0.5894118 ,\n",
       "       0.56431377, 0.55803925, 0.29764706, 0.6176471 , 0.9000001 ,\n",
       "       0.8905883 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.8937256 , 0.9000001 , 0.7588236 ,\n",
       "       0.1       , 0.10941177, 0.1       , 0.27254903, 0.8968628 ,\n",
       "       0.9000001 , 0.8968628 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.8968628 , 0.9000001 ,\n",
       "       0.39490196, 0.6992157 , 0.9000001 , 0.88431376, 0.8905883 ,\n",
       "       0.8905883 , 0.8905883 , 0.8905883 , 0.8905883 , 0.8905883 ,\n",
       "       0.8811765 , 0.9000001 , 0.64901966, 0.1       , 0.11568628,\n",
       "       0.1       , 0.38862744, 0.9000001 , 0.87803936, 0.8905883 ,\n",
       "       0.8905883 , 0.8905883 , 0.8905883 , 0.8905883 , 0.8905883 ,\n",
       "       0.887451  , 0.8968628 , 0.87490207, 0.23490196, 0.7650981 ,\n",
       "       0.9000001 , 0.8968628 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.8905883 , 0.9000001 ,\n",
       "       0.57686275, 0.1       , 0.11882353, 0.1       , 0.49843138,\n",
       "       0.9000001 , 0.887451  , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.8968628 , 0.9000001 ,\n",
       "       0.8215687 , 0.15333334, 0.8090196 , 0.9000001 , 0.8968628 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.887451  , 0.9000001 , 0.5172549 , 0.1       ,\n",
       "       0.11882353, 0.1       , 0.5894118 , 0.9000001 , 0.8905883 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.8937256 , 0.9000001 , 0.7494118 , 0.12196079,\n",
       "       0.8498039 , 0.9000001 , 0.8968628 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.887451  ,\n",
       "       0.9000001 , 0.48588237, 0.1       , 0.11882353, 0.1       ,\n",
       "       0.6333334 , 0.9000001 , 0.8905883 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.8905883 ,\n",
       "       0.9000001 , 0.6866667 , 0.1       , 0.8654902 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.887451  , 0.9000001 , 0.4576471 ,\n",
       "       0.1       , 0.1282353 , 0.1       , 0.6741177 , 0.9000001 ,\n",
       "       0.8905883 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.8905883 , 0.9000001 , 0.6333334 ,\n",
       "       0.1       , 0.8717648 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.887451  , 0.9000001 , 0.42941177, 0.1       , 0.11882353,\n",
       "       0.1       , 0.7211765 , 0.9000001 , 0.8905883 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.887451  , 0.9000001 , 0.60509807, 0.1       , 0.8811765 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.88431376, 0.9000001 ,\n",
       "       0.40745097, 0.1       , 0.29764706, 0.5611765 , 0.8717648 ,\n",
       "       0.9000001 , 0.8968628 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.887451  , 0.9000001 ,\n",
       "       0.5831373 , 0.1       , 0.8905883 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.8937256 , 0.9000001 , 0.6992157 , 0.73058826,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.887451  , 0.9000001 , 0.56745106, 0.1       ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.8937256 , 0.8905883 ,\n",
       "       0.8968628 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.887451  ,\n",
       "       0.9000001 , 0.5611765 , 0.1       , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.8937256 ,\n",
       "       0.8937256 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.887451  , 0.9000001 , 0.55803925,\n",
       "       0.1       , 0.8937256 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.887451  , 0.9000001 , 0.55490196, 0.1       , 0.887451  ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.8968628 , 0.8968628 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.887451  , 0.9000001 ,\n",
       "       0.55490196, 0.1       , 0.8811765 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.8937256 ,\n",
       "       0.8905883 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.887451  , 0.9000001 , 0.55490196, 0.1       ,\n",
       "       0.8717648 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.8529412 ,\n",
       "       0.8686275 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.887451  ,\n",
       "       0.9000001 , 0.55490196, 0.1       , 0.83411765, 0.9000001 ,\n",
       "       0.8968628 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.8968628 , 0.9000001 ,\n",
       "       0.78078437, 0.49529412, 0.18156862, 0.73372555, 0.9000001 ,\n",
       "       0.8937256 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.887451  , 0.9000001 , 0.55490196,\n",
       "       0.1       , 0.7901961 , 0.9000001 , 0.8968628 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.8905883 , 0.9000001 , 0.64901966, 0.10313725, 0.10627451,\n",
       "       0.1       , 0.7180393 , 0.9000001 , 0.8905883 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.887451  , 0.9000001 , 0.55490196, 0.1       , 0.74627453,\n",
       "       0.9000001 , 0.8937256 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.8905883 , 0.9000001 ,\n",
       "       0.62078434, 0.1       , 0.12509803, 0.1       , 0.702353  ,\n",
       "       0.9000001 , 0.8905883 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.887451  , 0.9000001 ,\n",
       "       0.55490196, 0.1       , 0.70549023, 0.9000001 , 0.8937256 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.8905883 , 0.9000001 , 0.6741177 , 0.1       ,\n",
       "       0.11254902, 0.1       , 0.6584314 , 0.9000001 , 0.8905883 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.887451  , 0.9000001 , 0.5611765 , 0.1       ,\n",
       "       0.6301961 , 0.9000001 , 0.8905883 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.8905883 ,\n",
       "       0.9000001 , 0.71176475, 0.1       , 0.11254902, 0.1       ,\n",
       "       0.60196084, 0.9000001 , 0.8905883 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.887451  ,\n",
       "       0.9000001 , 0.6145098 , 0.1       , 0.52980393, 0.9000001 ,\n",
       "       0.8905883 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.8937256 , 0.9000001 , 0.7556863 ,\n",
       "       0.1       , 0.10941177, 0.1       , 0.54862744, 0.9000001 ,\n",
       "       0.8905883 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.8905883 , 0.9000001 , 0.6866667 ,\n",
       "       0.1       , 0.4262745 , 0.9000001 , 0.8937256 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.8937256 , 0.9000001 , 0.7901961 , 0.10313725, 0.10941177,\n",
       "       0.1       , 0.47019607, 0.9000001 , 0.887451  , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.8937256 , 0.9000001 , 0.75254905, 0.12196079, 0.32274508,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.8968628 , 0.9000001 ,\n",
       "       0.8309805 , 0.16901961, 0.1       , 0.1       , 0.3792157 ,\n",
       "       0.9000001 , 0.887451  , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.8968628 , 0.9000001 ,\n",
       "       0.8247059 , 0.15333334, 0.20039216, 0.8686275 , 0.9000001 ,\n",
       "       0.8968628 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.8968628 , 0.9000001 , 0.87490207, 0.24117649,\n",
       "       0.1       , 0.1       , 0.3101961 , 0.9000001 , 0.8968628 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.8968628 , 0.9000001 , 0.87803936, 0.21607843,\n",
       "       0.1282353 , 0.7650981 , 0.9000001 , 0.8937256 , 0.9000001 ,\n",
       "       0.9000001 , 0.8968628 , 0.8968628 , 0.8968628 , 0.8937256 ,\n",
       "       0.887451  , 0.9000001 , 0.3070588 , 0.1       , 0.1       ,\n",
       "       0.22235295, 0.862353  , 0.9000001 , 0.8937256 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.8968628 , 0.8968628 , 0.8968628 ,\n",
       "       0.8968628 , 0.8968628 , 0.31960785, 0.1       , 0.62705886,\n",
       "       0.9000001 , 0.887451  , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.887451  , 0.9000001 ,\n",
       "       0.39490196, 0.1       , 0.10627451, 0.12196079, 0.8027451 ,\n",
       "       0.9000001 , 0.8937256 , 0.9000001 , 0.9000001 , 0.9000001 ,\n",
       "       0.9000001 , 0.9000001 , 0.9000001 , 0.8937256 , 0.9000001 ,\n",
       "       0.44509804, 0.1       , 0.49529412, 0.9000001 , 0.862353  ,\n",
       "       0.87490207, 0.85607845, 0.83411765, 0.8152942 , 0.78705883,\n",
       "       0.73372555, 0.6741177 , 0.6552942 , 0.3164706 , 0.1       ,\n",
       "       0.10941177, 0.1       , 0.6145098 , 0.83725494, 0.84666675,\n",
       "       0.8686275 , 0.87490207, 0.8686275 , 0.8498039 , 0.8309805 ,\n",
       "       0.8121569 , 0.7839216 , 0.8090196 , 0.45137253], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_grayscale(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/Mean_Variance_Image.png\" style=\"height: 75%;width: 75%; position: relative; right: 5%\">\n",
    "## Problem 1\n",
    "The first problem involves normalizing the features for your training and test data.\n",
    "\n",
    "Implement Min-Max scaling in the `normalize_grayscale()` function to a range of `a=0.1` and `b=0.9`. After scaling, the values of the pixels in the input data should range from 0.1 to 0.9.\n",
    "\n",
    "Since the raw notMNIST image data is in [grayscale](https://en.wikipedia.org/wiki/Grayscale), the current values range from a min of 0 to a max of 255.\n",
    "\n",
    "Min-Max Scaling:\n",
    "$\n",
    "X'=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}\n",
    "$\n",
    "\n",
    "*If you're having trouble solving problem 1, you can view the solution [here](https://github.com/udacity/deep-learning/blob/master/intro-to-tensorflow/intro_to_tensorflow_solution.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "# Problem 1 - Implement Min-Max scaling for grayscale image data\n",
    "def normalize_grayscale(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Min-Max scaling for grayscale image data\n",
    "    normalize=0.1+((image_data-image_data.min())*0.8/(image_data.max()-image_data.min()))\n",
    "    return normalize\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# Test Cases\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize_grayscale(np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 255])),\n",
    "    [0.1, 0.103137254902, 0.106274509804, 0.109411764706, 0.112549019608, 0.11568627451, 0.118823529412, 0.121960784314,\n",
    "     0.125098039216, 0.128235294118, 0.13137254902, 0.9],\n",
    "    decimal=3)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    normalize_grayscale(np.array([0, 1, 10, 20, 30, 40, 233, 244, 254,255])),\n",
    "    [0.1, 0.103137254902, 0.13137254902, 0.162745098039, 0.194117647059, 0.225490196078, 0.830980392157, 0.865490196078,\n",
    "     0.896862745098, 0.9])\n",
    "\n",
    "if not is_features_normal:\n",
    "    train_features = normalize_grayscale(train_features)\n",
    "    test_features = normalize_grayscale(test_features)\n",
    "    is_features_normal = True\n",
    "\n",
    "print('Tests Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 784)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels One-Hot Encoded\n"
     ]
    }
   ],
   "source": [
    "if not is_labels_encod:\n",
    "    # Turn labels into numbers and apply One-Hot Encoding\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(train_labels)\n",
    "    train_labels = encoder.transform(train_labels)\n",
    "    test_labels = encoder.transform(test_labels)\n",
    "\n",
    "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "    train_labels = train_labels.astype(np.float32)\n",
    "    test_labels = test_labels.astype(np.float32)\n",
    "    is_labels_encod = True\n",
    "\n",
    "print('Labels One-Hot Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features and labels randomized and split.\n"
     ]
    }
   ],
   "source": [
    "assert is_features_normal, 'You skipped the step to normalize the features'\n",
    "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
    "\n",
    "# Get randomized datasets for training and validation\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_size=0.05,\n",
    "    random_state=832289)\n",
    "\n",
    "print('Training features and labels randomized and split.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data to pickle file...\n",
      "Data cached in pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Save the data for easy access\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "if not os.path.isfile(pickle_file):\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open('notMNIST.pickle', 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'train_dataset': train_features,\n",
    "                    'train_labels': train_labels,\n",
    "                    'valid_dataset': valid_features,\n",
    "                    'valid_labels': valid_labels,\n",
    "                    'test_dataset': test_features,\n",
    "                    'test_labels': test_labels,\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "\n",
    "print('Data cached in pickle file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "All your progress is now saved to the pickle file.  If you need to leave and comeback to this lab, you no longer have to start from the beginning.  Just run the code block below and it will load all the data and modules required to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ania/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and modules loaded.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Load the modules\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload the data\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  pickle_data = pickle.load(f)\n",
    "  train_features = pickle_data['train_dataset']\n",
    "  train_labels = pickle_data['train_labels']\n",
    "  valid_features = pickle_data['valid_dataset']\n",
    "  valid_labels = pickle_data['valid_labels']\n",
    "  test_features = pickle_data['test_dataset']\n",
    "  test_labels = pickle_data['test_labels']\n",
    "  del pickle_data  # Free up memory\n",
    "\n",
    "print('Data and modules loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Problem 2\n",
    "\n",
    "Now it's time to build a simple neural network using TensorFlow. Here, your network will be just an input layer and an output layer.\n",
    "\n",
    "<img src=\"image/network_diagram.png\" style=\"height: 40%;width: 40%; position: relative; right: 10%\">\n",
    "\n",
    "For the input here the images have been flattened into a vector of $28 \\times 28 = 784$ features. Then, we're trying to predict the image digit so there are 10 output units, one for each label. Of course, feel free to add hidden layers if you want, but this notebook is built to guide you through a single layer network. \n",
    "\n",
    "For the neural network to train on your data, you need the following <a href=\"https://www.tensorflow.org/resources/dims_types.html#data-types\">float32</a> tensors:\n",
    " - `features`\n",
    "  - Placeholder tensor for feature data (`train_features`/`valid_features`/`test_features`)\n",
    " - `labels`\n",
    "  - Placeholder tensor for label data (`train_labels`/`valid_labels`/`test_labels`)\n",
    " - `weights`\n",
    "  - Variable Tensor with random numbers from a truncated normal distribution.\n",
    "    - See <a href=\"https://www.tensorflow.org/api_docs/python/constant_op.html#truncated_normal\">`tf.truncated_normal()` documentation</a> for help.\n",
    " - `biases`\n",
    "  - Variable Tensor with all zeros.\n",
    "    - See <a href=\"https://www.tensorflow.org/api_docs/python/constant_op.html#zeros\"> `tf.zeros()` documentation</a> for help.\n",
    "\n",
    "*If you're having trouble solving problem 2, review \"TensorFlow Linear Function\" section of the class.  If that doesn't help, the solution for this problem is available [here](intro_to_tensorflow_solution.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "Tests Passed!\n"
     ]
    }
   ],
   "source": [
    "# All the pixels in the image (28 * 28 = 784)\n",
    "features_count = 784\n",
    "# All the labels\n",
    "labels_count = 10\n",
    "\n",
    "# TODO: Set the features and labels tensors\n",
    "features = tf.placeholder(tf.float32,[None,features_count])\n",
    "labels = tf.placeholder(tf.float32,[None,labels_count])\n",
    "\n",
    "# TODO: Set the weights and biases tensors\n",
    "weights = tf.Variable(tf.truncated_normal([features_count,labels_count]))\n",
    "biases = tf.Variable(tf.zeros([labels_count]))\n",
    "\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "\n",
    "#Test Cases\n",
    "from tensorflow.python.ops.variables import Variable\n",
    "\n",
    "assert features._op.name.startswith('Placeholder'), 'features must be a placeholder'\n",
    "assert labels._op.name.startswith('Placeholder'), 'labels must be a placeholder'\n",
    "assert isinstance(weights, Variable), 'weights must be a TensorFlow variable'\n",
    "assert isinstance(biases, Variable), 'biases must be a TensorFlow variable'\n",
    "\n",
    "assert features._shape == None or (\\\n",
    "    features._shape.dims[0].value is None and\\\n",
    "    features._shape.dims[1].value in [None, 784]), 'The shape of features is incorrect'\n",
    "assert labels._shape  == None or (\\\n",
    "    labels._shape.dims[0].value is None and\\\n",
    "    labels._shape.dims[1].value in [None, 10]), 'The shape of labels is incorrect'\n",
    "assert weights._variable._shape == (784, 10), 'The shape of weights is incorrect'\n",
    "assert biases._variable._shape == (10), 'The shape of biases is incorrect'\n",
    "\n",
    "assert features._dtype == tf.float32, 'features must be type float32'\n",
    "assert labels._dtype == tf.float32, 'labels must be type float32'\n",
    "\n",
    "# Feed dicts for training, validation, and test session\n",
    "train_feed_dict = {features: train_features, labels: train_labels}\n",
    "valid_feed_dict = {features: valid_features, labels: valid_labels}\n",
    "test_feed_dict = {features: test_features, labels: test_labels}\n",
    "\n",
    "# Linear Function WX + b\n",
    "logits = tf.matmul(features, weights) + biases\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy\n",
    "cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1) #sum over labels/predc per sample*log(prediction)\n",
    "\n",
    "# Training loss\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Create an operation that initializes all variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Test Cases   1 iteration\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    session.run(loss, feed_dict=train_feed_dict)\n",
    "    session.run(loss, feed_dict=valid_feed_dict)\n",
    "    session.run(loss, feed_dict=test_feed_dict)\n",
    "    biases_data = session.run(biases)\n",
    "\n",
    "assert not np.count_nonzero(biases_data), 'biases must be zeros'\n",
    "\n",
    "print('Tests Passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy function created.\n"
     ]
    }
   ],
   "source": [
    "# Determine if the predictions are correct\n",
    "is_correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "\n",
    "print('Accuracy function created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/Learn_Rate_Tune_Image.png\" style=\"height: 70%;width: 70%\">\n",
    "## Problem 3\n",
    "Below are 2 parameter configurations for training the neural network. In each configuration, one of the parameters has multiple options. For each configuration, choose the option that gives the best acccuracy.\n",
    "\n",
    "Parameter configurations:\n",
    "\n",
    "Configuration 1\n",
    "* **Epochs:** 1\n",
    "* **Learning Rate:**\n",
    "  * 0.8\n",
    "  * 0.5\n",
    "  * 0.1\n",
    "  * 0.05\n",
    "  * 0.01\n",
    "\n",
    "Configuration 2\n",
    "* **Epochs:**\n",
    "  * 1\n",
    "  * 2\n",
    "  * 3\n",
    "  * 4\n",
    "  * 5\n",
    "* **Learning Rate:** 0.2\n",
    "\n",
    "The code will print out a Loss and Accuracy graph, so you can see how well the neural network performed.\n",
    "\n",
    "*If you're having trouble solving problem 3, you can view the solution [here](intro_to_tensorflow_solution.ipynb).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch  1/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  1/10:   1%|          | 1/112 [00:00<00:13,  8.14batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  1/10:  41%|████      | 46/112 [00:00<00:00, 205.73batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  1/10:  54%|█████▎    | 60/112 [00:00<00:00, 183.50batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  1/10:  88%|████████▊ | 99/112 [00:00<00:00, 230.21batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  1/10: 100%|██████████| 112/112 [00:00<00:00, 215.22batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  2/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  2/10:   5%|▌         | 6/112 [00:00<00:01, 58.13batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  2/10:  40%|████      | 45/112 [00:00<00:00, 214.15batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  2/10:  52%|█████▏    | 58/112 [00:00<00:00, 187.07batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  2/10:  90%|█████████ | 101/112 [00:00<00:00, 215.39batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  2/10: 100%|██████████| 112/112 [00:00<00:00, 223.77batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  3/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  3/10:  32%|███▏      | 36/112 [00:00<00:00, 354.38batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  3/10:  46%|████▌     | 51/112 [00:00<00:00, 248.08batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  3/10:  90%|█████████ | 101/112 [00:00<00:00, 262.72batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  3/10: 100%|██████████| 112/112 [00:00<00:00, 262.44batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  4/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  4/10:  21%|██        | 23/112 [00:00<00:00, 227.02batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  4/10:  46%|████▌     | 51/112 [00:00<00:00, 238.43batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  4/10:  90%|█████████ | 101/112 [00:00<00:00, 265.14batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  4/10: 100%|██████████| 112/112 [00:00<00:00, 268.41batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  5/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  5/10:   6%|▋         | 7/112 [00:00<00:01, 68.52batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  5/10:  46%|████▌     | 51/112 [00:00<00:00, 185.07batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  5/10:  91%|█████████ | 102/112 [00:00<00:00, 270.97batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  5/10: 100%|██████████| 112/112 [00:00<00:00, 287.49batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  6/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  6/10:   1%|          | 1/112 [00:00<00:12,  8.84batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  6/10:  46%|████▌     | 51/112 [00:00<00:00, 176.47batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  6/10:  90%|█████████ | 101/112 [00:00<00:00, 228.32batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  6/10: 100%|██████████| 112/112 [00:00<00:00, 241.98batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  7/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  7/10:  20%|█▉        | 22/112 [00:00<00:00, 208.80batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  7/10:  46%|████▌     | 51/112 [00:00<00:00, 225.80batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  7/10:  87%|████████▋ | 97/112 [00:00<00:00, 295.88batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  7/10: 100%|██████████| 112/112 [00:00<00:00, 216.86batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  8/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  8/10:   1%|          | 1/112 [00:00<00:12,  8.76batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  8/10:  46%|████▌     | 51/112 [00:00<00:00, 217.94batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  8/10:  82%|████████▏ | 92/112 [00:00<00:00, 276.63batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  8/10: 100%|██████████| 112/112 [00:00<00:00, 263.91batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  9/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  9/10:   4%|▍         | 5/112 [00:00<00:02, 49.14batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  9/10:  46%|████▌     | 51/112 [00:00<00:00, 238.88batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  9/10:  90%|█████████ | 101/112 [00:00<00:00, 312.55batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  9/10: 100%|██████████| 112/112 [00:00<00:00, 333.32batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 10/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 10/10:  18%|█▊        | 20/112 [00:00<00:00, 196.69batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 10/10:  46%|████▌     | 51/112 [00:00<00:00, 233.08batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 10/10:  90%|█████████ | 101/112 [00:00<00:00, 283.95batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 10/10: 100%|██████████| 112/112 [00:00<00:00, 302.01batches/s]\u001b[A\u001b[A"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at 0.7253333330154419\n"
     ]
    }
   ],
   "source": [
    "# Change if you have memory restrictions\n",
    "batch_size = 128\n",
    "\n",
    "# TODO: Find the best parameters for each configuration\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "\n",
    "\n",
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)    \n",
    "\n",
    "# The accuracy measured against the validation set\n",
    "validation_accuracy = 0.0\n",
    "\n",
    "# Measurements use for graphing loss and accuracy\n",
    "log_batch_step = 50\n",
    "batches = []\n",
    "loss_batch = []\n",
    "train_acc_batch = []\n",
    "valid_acc_batch = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        \n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "\n",
    "            # Run optimizer and get loss\n",
    "            _, l = session.run(\n",
    "                [optimizer, loss],\n",
    "                feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "            # Log every 50 batches\n",
    "            if not batch_i % log_batch_step:\n",
    "                # Calculate Training and Validation accuracy\n",
    "                training_accuracy = session.run(accuracy, feed_dict=train_feed_dict)\n",
    "                validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
    "\n",
    "                # Log batches\n",
    "                previous_batch = batches[-1] if batches else 0\n",
    "                batches.append(log_batch_step + previous_batch)\n",
    "                loss_batch.append(l)\n",
    "                train_acc_batch.append(training_accuracy)\n",
    "                valid_acc_batch.append(validation_accuracy)\n",
    "\n",
    "        # Check accuracy against Validation data\n",
    "        validation_accuracy = session.run(accuracy, feed_dict=valid_feed_dict)\n",
    "\n",
    "loss_plot = plt.subplot(211)\n",
    "loss_plot.set_title('Loss')\n",
    "loss_plot.plot(batches, loss_batch, 'g')\n",
    "loss_plot.set_xlim([batches[0], batches[-1]])\n",
    "acc_plot = plt.subplot(212)\n",
    "acc_plot.set_title('Accuracy')\n",
    "acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')\n",
    "acc_plot.plot(batches, valid_acc_batch, 'x', label='Validation Accuracy')\n",
    "acc_plot.set_ylim([0, 1.0])\n",
    "acc_plot.set_xlim([batches[0], batches[-1]])\n",
    "acc_plot.legend(loc=4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Validation accuracy at {}'.format(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch 1 , learning rate 0.5, 70%\n",
    "#epoch 4, lr_ 0.2      72%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "You're going to test your model against your hold out dataset/testing data.  This will give you a good indicator of how well the model will do in the real world.  You should have a test accuracy of at least 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch  1/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  1/10:  23%|██▎       | 26/112 [00:00<00:00, 251.48batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  1/10:  71%|███████▏  | 80/112 [00:00<00:00, 391.59batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  1/10: 100%|██████████| 112/112 [00:00<00:00, 433.35batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  2/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  2/10:  63%|██████▎   | 71/112 [00:00<00:00, 655.43batches/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test accuracy 0.53333336 0.5867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch  2/10:  95%|█████████▍| 106/112 [00:00<00:00, 511.62batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  2/10: 100%|██████████| 112/112 [00:00<00:00, 495.84batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  3/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  3/10:  38%|███▊      | 42/112 [00:00<00:00, 412.69batches/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test accuracy 0.63614035 0.7027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch  3/10:  92%|█████████▏| 103/112 [00:00<00:00, 509.52batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  3/10: 100%|██████████| 112/112 [00:00<00:00, 507.83batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  4/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  4/10:  38%|███▊      | 42/112 [00:00<00:00, 415.92batches/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test accuracy 0.6808421 0.7466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch  4/10:  94%|█████████▍| 105/112 [00:00<00:00, 514.21batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  4/10: 100%|██████████| 112/112 [00:00<00:00, 477.53batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  5/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  5/10:  38%|███▊      | 42/112 [00:00<00:00, 416.46batches/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test accuracy 0.70182455 0.7696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch  5/10:  89%|████████▉ | 100/112 [00:00<00:00, 491.29batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  5/10: 100%|██████████| 112/112 [00:00<00:00, 475.79batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  6/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  6/10:  63%|██████▎   | 71/112 [00:00<00:00, 669.56batches/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test accuracy 0.7157895 0.7832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6/10: 100%|██████████| 112/112 [00:00<00:00, 538.62batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  7/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  7/10:  50%|█████     | 56/112 [00:00<00:00, 550.87batches/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test accuracy 0.72533333 0.7924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch  7/10:  94%|█████████▍| 105/112 [00:00<00:00, 508.67batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  7/10: 100%|██████████| 112/112 [00:00<00:00, 504.84batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  8/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  8/10:  45%|████▍     | 50/112 [00:00<00:00, 486.61batches/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test accuracy 0.73263156 0.7987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch  8/10:  72%|███████▏  | 81/112 [00:00<00:00, 402.27batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  8/10:  98%|█████████▊| 110/112 [00:00<00:00, 358.56batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  8/10: 100%|██████████| 112/112 [00:00<00:00, 345.64batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  9/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  9/10:  33%|███▎      | 37/112 [00:00<00:00, 361.26batches/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test accuracy 0.7364912 0.8042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch  9/10:  84%|████████▍ | 94/112 [00:00<00:00, 461.08batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch  9/10: 100%|██████████| 112/112 [00:00<00:00, 447.64batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 10/10:   0%|          | 0/112 [00:00<?, ?batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 10/10:  62%|██████▎   | 70/112 [00:00<00:00, 679.36batches/s]\u001b[A\u001b[A\n",
      "\n",
      "Epoch 10/10: 100%|██████████| 112/112 [00:00<00:00, 683.03batches/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test accuracy 0.7413333 0.808\n",
      "train/test accuracy 0.74645615 0.8118\n",
      "Nice Job! Test Accuracy is 0.8118000030517578\n"
     ]
    }
   ],
   "source": [
    "### DON'T MODIFY ANYTHING BELOW ###\n",
    "# The accuracy measured against the test set    why we are iterating in time!, aren't weight already set of weights from training?\n",
    "test_accuracy = 0.0\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    session.run(init)\n",
    "    batch_count = int(math.ceil(len(train_features)/batch_size))\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        # Progress bar\n",
    "        batches_pbar = tqdm(range(batch_count), desc='Epoch {:>2}/{}'.format(epoch_i+1, epochs), unit='batches')\n",
    "        \n",
    "        # The training cycle\n",
    "        for batch_i in batches_pbar:\n",
    "            # Get a batch of training features and labels\n",
    "            batch_start = batch_i*batch_size\n",
    "            batch_features = train_features[batch_start:batch_start + batch_size]\n",
    "            batch_labels = train_labels[batch_start:batch_start + batch_size]\n",
    "\n",
    "            # Run optimizer\n",
    "            _ = session.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Check accuracy against Test data\n",
    "        test_accuracy = session.run(accuracy, feed_dict=test_feed_dict)\n",
    "        print('train/test accuracy',session.run(accuracy, feed_dict=train_feed_dict),session.run(accuracy, feed_dict=test_feed_dict)) \n",
    "\n",
    "assert test_accuracy >= 0.80, 'Test accuracy at {}, should be equal to or greater than 0.80'.format(test_accuracy)\n",
    "print('Nice Job! Test Accuracy is {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple layers\n",
    "Good job!  You built a one layer TensorFlow network!  However, you might want to build more than one layer.  This is deep learning after all!  In the next section, you will start to satisfy your need for more layers."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
